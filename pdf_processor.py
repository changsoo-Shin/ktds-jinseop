"""
PDF ì²˜ë¦¬ ë° ë²¡í„°í™” ëª¨ë“ˆ
Doclingì„ ì‚¬ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° FAISS ë²¡í„° DB êµ¬ì¶•
"""

import os
import tempfile
from typing import List, Dict, Any, Optional
import numpy as np
from pathlib import Path
import hashlib
from datetime import datetime
import json
import re
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# Docling ê´€ë ¨ import
try:
    from docling.document_converter import DocumentConverter
except ImportError:
    logger.error("Doclingì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install doclingì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    DocumentConverter = None

# FAISS ê´€ë ¨ import
try:
    import faiss
    from sentence_transformers import SentenceTransformer
except ImportError:
    logger.error("FAISS ë˜ëŠ” sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    logger.error("pip install faiss-cpu sentence-transformersë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    faiss = None
    SentenceTransformer = None

class PDFProcessor:
    """PDF ì²˜ë¦¬ ë° ë²¡í„°í™” í´ë˜ìŠ¤"""
    
    def __init__(self, vector_db_path: str = "faiss_vector_db"):
        self.vector_db_path = Path(vector_db_path)
        self.vector_db_path.mkdir(exist_ok=True)
        
        # ë¬¸ì œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.questions_dir = Path("extracted_questions")
        self.questions_dir.mkdir(exist_ok=True)
        
        # ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = None
        self.index = None
        self.documents = []
        self.metadata = []
        
        self._initialize_models()
    
    def _initialize_models(self):
        """ë²¡í„° ëª¨ë¸ ë° FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if SentenceTransformer is None or faiss is None:
            logger.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"ğŸ”§ [INFO] ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
            if device == 'cuda':
                logger.info(f"ğŸ”§ [INFO] GPU ëª¨ë¸: {torch.cuda.get_device_name(0)}")
            
            # í•œêµ­ì–´ì— íŠ¹í™”ëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)
            
            # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (L2 ê±°ë¦¬ ê¸°ë°˜)
            dimension = self.embedding_model.get_sentence_embedding_dimension()
            
            # FAISSëŠ” CPU ì‚¬ìš© (Python 3.12ì—ì„œ GPU FAISS ë¯¸ì§€ì›)
            self.index = faiss.IndexFlatL2(dimension)
            logger.info(f"ğŸ”§ [INFO] FAISS CPU ì¸ë±ìŠ¤ ì‚¬ìš© (Python 3.12)")
            
            logger.info(f"âœ… ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {dimension})")
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def process_pdf(self, pdf_file_path: str, subject: str = "ì •ë³´ì‹œìŠ¤í…œê°ë¦¬ì‚¬", original_filename: str = None) -> Dict[str, Any]:
        """PDF íŒŒì¼ ì²˜ë¦¬ ë° ë²¡í„°í™”"""
        logger.info(f"\nğŸ“„ [PDF ì²˜ë¦¬] íŒŒì¼: {pdf_file_path}")
        
        if DocumentConverter is None:
            return {"success": False, "error": "Docling ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        try:
            # Docling GPU ì„¤ì •
            import os
            import torch
            if torch.cuda.is_available():
                os.environ['DOCLING_ACCELERATOR'] = 'cuda'
                logger.info(f"ğŸ”§ [INFO] Docling GPU ì„¤ì •: cuda")
            else:
                os.environ['DOCLING_ACCELERATOR'] = 'cpu'
                logger.info(f"ğŸ”§ [INFO] Docling GPU ì„¤ì •: cpu")
            
            converter = DocumentConverter()
            result = converter.convert(pdf_file_path)
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Markdown ê¸°ì¤€)
            full_text = result.document.export_to_markdown()
            
            # ë¬¸ì œ ì¶”ì¶œ ë° ì €ì¥
            extracted_questions = self._extract_questions_from_text(full_text, subject, original_filename)
            logger.info(f"ğŸ“ [ë¬¸ì œ ì¶”ì¶œ] {len(extracted_questions)}ê°œì˜ ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ")
            
            # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ë° ë²¡í„°í™”ëŠ” ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
            text_chunks = self._extract_and_chunk_text_from_text(full_text, subject)
            if not text_chunks:
                return {"success": False, "error": "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # ì‹¤ì œ íŒŒì¼ëª… ì‚¬ìš© (ì—†ìœ¼ë©´ ì„ì‹œ íŒŒì¼ëª… ì‚¬ìš©)
            filename_to_use = original_filename if original_filename is not None else Path(pdf_file_path).name
            self._vectorize_and_store(text_chunks, subject, filename_to_use)
            self._save_metadata()
            logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ - {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±")
            return {
                "success": True,
                "chunks_count": len(text_chunks),
                "questions_count": len(extracted_questions),
                "subject": subject,
                "filename": filename_to_use
            }
        except Exception as e:
            error_msg = f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(f"âŒ {error_msg}")
            return {"success": False, "error": error_msg}
    
    def _extract_questions_from_text(self, full_text: str, subject: str, original_filename: str = None) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œë§Œ ì¶”ì¶œ"""
        questions = []
        lines = full_text.split('\n')
        
        # ë¬¸ì œ ì‹œì‘ íŒ¨í„´ë“¤
        question_patterns = [
            r'^(\d+)\s*[\.\)]\s*',  # 1. ë˜ëŠ” 1)
            r'^ë¬¸ì œ\s*(\d+)\s*[\.\)]?\s*',  # ë¬¸ì œ 1. ë˜ëŠ” ë¬¸ì œ 1)
            r'^ë¬¸í•­\s*(\d+)\s*[\.\)]?\s*',  # ë¬¸í•­ 1. ë˜ëŠ” ë¬¸í•­ 1)
            r'^(\d+)\s*ë²ˆ\s*',  # 1ë²ˆ
            r'^ë¬¸ì œ\s*(\d+)\s*ë²ˆ\s*',  # ë¬¸ì œ 1ë²ˆ
            r'^ë¬¸í•­\s*(\d+)\s*ë²ˆ\s*',  # ë¬¸í•­ 1ë²ˆ
            r'^(\d+)\s*[\.\)]\s*[ê°€-í£]',  # 1. ë‹¤ìŒ ì¤‘ ë˜ëŠ” 1) ë‹¤ìŒ ì¤‘
        ]
        
        current_question = None
        current_question_lines = []
        question_number = None
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
            
            # ë¬¸ì œ ì‹œì‘ ê°ì§€
            is_question_start = False
            detected_number = None
            
            for pattern in question_patterns:
                match = re.match(pattern, line)
                if match:
                    detected_number = match.group(1)
                    # ì—°ë„ê°€ ì•„ë‹Œ ë¬¸ì œ ë²ˆí˜¸ì¸ì§€ í™•ì¸ (1-200 ë²”ìœ„)
                    if detected_number.isdigit() and 1 <= int(detected_number) <= 200:
                        is_question_start = True
                        break
            
            if is_question_start:
                # ì´ì „ ë¬¸ì œ ì €ì¥
                if current_question and current_question_lines:
                    question_text = '\n'.join(current_question_lines).strip()
                    if len(question_text) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        questions.append({
                            "number": question_number,
                            "text": question_text,
                            "start_line": current_question,
                            "end_line": i - 1
                        })
                
                # ìƒˆ ë¬¸ì œ ì‹œì‘
                current_question = i
                current_question_lines = [line]
                question_number = detected_number
            else:
                # í˜„ì¬ ë¬¸ì œì— ë¼ì¸ ì¶”ê°€
                if current_question is not None:
                    current_question_lines.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì œ ì €ì¥
        if current_question and current_question_lines:
            question_text = '\n'.join(current_question_lines).strip()
            if len(question_text) > 50:
                questions.append({
                    "number": question_number,
                    "text": question_text,
                    "start_line": current_question,
                    "end_line": len(lines) - 1
                })
        
        # ë¬¸ì œ ì €ì¥
        if questions:
            self._save_questions(questions, subject, original_filename)
        
        return questions
    
    def _save_questions(self, questions: List[Dict[str, Any]], subject: str, original_filename: str = None):
        """ì¶”ì¶œëœ ë¬¸ì œë¥¼ txtì™€ jsonìœ¼ë¡œ ì €ì¥"""
        try:
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = original_filename.replace('.pdf', '') if original_filename else f"{subject}_{timestamp}"
            
            # JSON íŒŒì¼ ì €ì¥
            json_data = {
                "subject": subject,
                "source_file": original_filename,
                "extraction_date": datetime.now().isoformat(),
                "total_questions": len(questions),
                "questions": questions
            }
            
            json_file = self.questions_dir / f"{base_filename}_questions.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            # TXT íŒŒì¼ ì €ì¥
            txt_file = self.questions_dir / f"{base_filename}_questions.txt"
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(f"# {subject} ê¸°ì¶œë¬¸ì œ\n")
                f.write(f"# ì¶œì²˜: {original_filename}\n")
                f.write(f"# ì¶”ì¶œì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# ì´ ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ\n\n")
                
                for i, question in enumerate(questions, 1):
                    f.write(f"=== ë¬¸ì œ {question['number']} ===\n")
                    f.write(f"{question['text']}\n\n")
            
            logger.info(f"âœ… ë¬¸ì œ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"   ğŸ“„ JSON: {json_file}")
            logger.info(f"   ğŸ“„ TXT: {txt_file}")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _extract_and_chunk_text_from_text(self, full_text: str, subject: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸(ë¬¸ìì—´)ì—ì„œ ì²­í¬ ë¶„í• """
        chunks = []
        try:
            # í‘œ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬
            lines = full_text.split('\n')
            current_chunk_lines = []
            current_length = 0
            in_table = False
            table_start_line = -1
            
            for i, line in enumerate(lines):
                # í‘œ ì‹œì‘/ë ê°ì§€
                is_table_line = line.strip().startswith('|') or ('|' in line.strip() and len(line.strip()) > 10)
                
                if is_table_line and not in_table:
                    # í‘œ ì‹œì‘
                    in_table = True
                    table_start_line = i
                    # í‘œ ì‹œì‘ ì „ê¹Œì§€ì˜ ì²­í¬ ì €ì¥
                    if current_chunk_lines and len('\n'.join(current_chunk_lines).strip()) >= 100:
                        chunk_text = '\n'.join(current_chunk_lines)
                        chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_{chunk_text[:50]}".encode()).hexdigest()
                        chunks.append({
                            "id": chunk_id,
                            "text": chunk_text.strip(),
                            "start_pos": len(chunks) * 1000,
                            "end_pos": len(chunks) * 1000 + len(chunk_text),
                            "subject": subject,
                            "created_at": datetime.now().isoformat()
                        })
                    current_chunk_lines = []
                    current_length = 0
                
                elif not is_table_line and in_table:
                    # í‘œ ë
                    in_table = False
                    # í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì €ì¥
                    table_lines = lines[table_start_line:i]
                    table_text = '\n'.join(table_lines)
                    if len(table_text.strip()) >= 50:
                        chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_table_{table_start_line}".encode()).hexdigest()
                        chunks.append({
                            "id": chunk_id,
                            "text": table_text.strip(),
                            "start_pos": len(chunks) * 1000,
                            "end_pos": len(chunks) * 1000 + len(table_text),
                            "subject": subject,
                            "created_at": datetime.now().isoformat(),
                            "is_table": True
                        })
                    current_chunk_lines = []
                    current_length = 0
                
                # í˜„ì¬ ë¼ì¸ ì¶”ê°€
                if not in_table:
                    current_chunk_lines.append(line)
                    current_length += len(line) + 1
                    
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸° ì²´í¬
                    if current_length >= 1000:
                        chunk_text = '\n'.join(current_chunk_lines)
                        if len(chunk_text.strip()) >= 100:
                            chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_{chunk_text[:50]}".encode()).hexdigest()
                            chunks.append({
                                "id": chunk_id,
                                "text": chunk_text.strip(),
                                "start_pos": len(chunks) * 1000,
                                "end_pos": len(chunks) * 1000 + len(chunk_text),
                                "subject": subject,
                                "created_at": datetime.now().isoformat()
                            })
                        current_chunk_lines = []
                        current_length = 0
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if current_chunk_lines:
                chunk_text = '\n'.join(current_chunk_lines)
                if len(chunk_text.strip()) >= 100:
                    chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_{chunk_text[:50]}".encode()).hexdigest()
                    chunks.append({
                        "id": chunk_id,
                        "text": chunk_text.strip(),
                        "start_pos": len(chunks) * 1000,
                        "end_pos": len(chunks) * 1000 + len(chunk_text),
                        "subject": subject,
                        "created_at": datetime.now().isoformat()
                    })
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return chunks
    
    def _vectorize_and_store(self, chunks: List[Dict[str, Any]], subject: str, pdf_file_path: str):
        """ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ê³  FAISSì— ì €ì¥"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [chunk["text"] for chunk in chunks]
            
            # ë²¡í„°í™”
            logger.info("ğŸ”„ í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
            embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embeddings.astype('float32'))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            for i, chunk in enumerate(chunks):
                chunk["embedding_id"] = len(self.documents) + i
                chunk["pdf_source"] = Path(pdf_file_path).name
                self.documents.append(chunk["text"])
                self.metadata.append(chunk)
            
            logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ë²¡í„°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")
    
    def search_similar_chunks(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰"""
        if self.embedding_model is None or self.index is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.embedding_model.encode([query])
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(query_embedding.astype('float32'), n_results)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(self.metadata):
                    result = {
                        "rank": i + 1,
                        "distance": float(distance),
                        "text": self.documents[idx],
                        "metadata": self.metadata[idx]
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def get_chunks_by_subject(self, subject: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ê³¼ëª©ë³„ ì²­í¬ ì¡°íšŒ"""
        results = []
        
        for metadata in self.metadata:
            if metadata.get("subject") == subject:
                results.append({
                    "text": metadata.get("text", ""),
                    "metadata": metadata
                })
                if len(results) >= n_results:
                    break
        
        return results
    
    def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            metadata_file = self.vector_db_path / "metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_chunks": len(self.documents),
                    "metadata": self.metadata,
                    "last_updated": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            if self.index:
                index_file = self.vector_db_path / "faiss_index.bin"
                faiss.write_index(self.index, str(index_file))
            
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë° ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def load_existing_data(self):
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        try:
            metadata_file = self.vector_db_path / "metadata.json"
            index_file = self.vector_db_path / "faiss_index.bin"
            
            if metadata_file.exists() and index_file.exists():
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.metadata = data.get("metadata", [])
                    self.documents = [meta.get("text", "") for meta in self.metadata]
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if faiss:
                    self.index = faiss.read_index(str(index_file))
                
                logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - {len(self.documents)}ê°œ ì²­í¬")
                return True
            
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„ ì •ë³´"""
        stats = {
            "total_chunks": len(self.documents),
            "total_metadata": len(self.metadata),
            "index_size": self.index.ntotal if self.index else 0,
            "subjects": list(set([meta.get("subject", "") for meta in self.metadata if meta.get("subject")])),
            "pdf_sources": list(set([meta.get("pdf_source", "") for meta in self.metadata if meta.get("pdf_source")]))
        }
        return stats
    
    def clear_all_data(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        try:
            self.documents = []
            self.metadata = []
            if self.index:
                self.index.reset()
            
            # íŒŒì¼ ì‚­ì œ
            metadata_file = self.vector_db_path / "metadata.json"
            index_file = self.vector_db_path / "faiss_index.bin"
            
            if metadata_file.exists():
                metadata_file.unlink()
            if index_file.exists():
                index_file.unlink()
            
            logger.info("âœ… ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_extracted_questions(self, subject: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì‹œí—˜ì˜ ì¶”ì¶œëœ ë¬¸ì œ ëª©ë¡ ì¡°íšŒ"""
        questions = []
        
        try:
            # questions_dirì—ì„œ í•´ë‹¹ ì‹œí—˜ì˜ JSON íŒŒì¼ë“¤ ì°¾ê¸°
            for json_file in self.questions_dir.glob(f"*_questions.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    # í•´ë‹¹ ì‹œí—˜ì˜ ë¬¸ì œë§Œ í•„í„°ë§
                    if data.get("subject") == subject:
                        source_file = data.get("source_file", "unknown")
                        # ê° ë¬¸ì œì— ì¶œì²˜ íŒŒì¼ ì •ë³´ ì¶”ê°€
                        for question in data.get("questions", []):
                            question["source_file"] = source_file
                        questions.extend(data.get("questions", []))
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({json_file}): {e}")
                    continue
            
            # ë¬¸ì œ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            questions.sort(key=lambda x: int(x.get("number", 0)) if x.get("number", "0").isdigit() else 0)
            
            logger.info(f"âœ… {subject} ì‹œí—˜ì˜ ì¶”ì¶œëœ ë¬¸ì œ {len(questions)}ê°œ ë¡œë“œ ì™„ë£Œ")
            return questions
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œëœ ë¬¸ì œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def search_extracted_questions(self, query: str, subject: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ì¶”ì¶œëœ ë¬¸ì œì—ì„œ ê²€ìƒ‰"""
        questions = self.get_extracted_questions(subject)
        if not questions:
            return []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        query_lower = query.lower()
        matched_questions = []
        
        for question in questions:
            question_text = question.get("text", "").lower()
            if query_lower in question_text:
                matched_questions.append(question)
        
        # ìƒìœ„ n_resultsê°œ ë°˜í™˜
        return matched_questions[:n_results]
    
    def get_random_extracted_question(self, subject: str) -> Optional[Dict[str, Any]]:
        """ì¶”ì¶œëœ ë¬¸ì œì—ì„œ ëœë¤ ì„ íƒ"""
        questions = self.get_extracted_questions(subject)
        if not questions:
            return None
        
        import random
        return random.choice(questions)
    
    def get_extracted_question_by_number(self, subject: str, question_number: str) -> Optional[Dict[str, Any]]:
        """ë¬¸ì œ ë²ˆí˜¸ë¡œ íŠ¹ì • ë¬¸ì œ ì¡°íšŒ"""
        questions = self.get_extracted_questions(subject)
        
        for question in questions:
            if question.get("number") == question_number:
                return question
        
        return None
    
    def search_extracted_questions_semantic(self, query: str, subject: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ì¶”ì¶œëœ ë¬¸ì œì—ì„œ semantic ê²€ìƒ‰"""
        questions = self.get_extracted_questions(subject)
        if not questions:
            return []
        
        if self.embedding_model is None:
            logger.warning("âš ï¸ ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.search_extracted_questions(query, subject, n_results)
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.embedding_model.encode([query])
            
            # ëª¨ë“  ë¬¸ì œ í…ìŠ¤íŠ¸ ë²¡í„°í™”
            question_texts = [q["text"] for q in questions]
            question_embeddings = self.embedding_model.encode(question_texts)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, question_embeddings)[0]
            
            # ìœ ì‚¬ë„ì™€ í•¨ê»˜ ê²°ê³¼ êµ¬ì„±
            results_with_scores = []
            for i, (question, similarity) in enumerate(zip(questions, similarities)):
                results_with_scores.append({
                    "question": question,
                    "score": float(similarity),
                    "rank": i + 1
                })
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            results_with_scores.sort(key=lambda x: x["score"], reverse=True)
            
            # ìƒìœ„ n_resultsê°œ ë°˜í™˜
            top_results = results_with_scores[:n_results]
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for result in top_results:
                question_data = result["question"]
                results.append({
                    "content": question_data["text"],
                    "metadata": {
                        "type": "extracted_question",
                        "subject": subject,
                        "question_number": question_data["number"],
                        "pdf_source": "ì¶”ì¶œëœ ê¸°ì¶œë¬¸ì œ",
                        "score": result["score"],
                        "rank": result["rank"]
                    },
                    "score": result["score"]
                })
            
            logger.info(f"âœ… ì¶”ì¶œëœ ë¬¸ì œ semantic ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œëœ ë¬¸ì œ semantic ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
            return self.search_extracted_questions(query, subject, n_results)

# ì „ì—­ PDF í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
pdf_processor = PDFProcessor() 