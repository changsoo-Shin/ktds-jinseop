"""
PDF ì²˜ë¦¬ ë° ë²¡í„°í™” ëª¨ë“ˆ
Doclingì„ ì‚¬ìš©í•œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° FAISS ë²¡í„° DB êµ¬ì¶•
"""

import os
import tempfile
from typing import List, Dict, Any, Optional
import numpy as np
from pathlib import Path
import hashlib
from datetime import datetime
import json
import re
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# Docling ê´€ë ¨ import
try:
    from docling.document_converter import DocumentConverter
except ImportError:
    logger.error("Doclingì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install doclingì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    DocumentConverter = None

# FAISS ê´€ë ¨ import
try:
    import faiss
    from sentence_transformers import SentenceTransformer
except ImportError:
    logger.error("FAISS ë˜ëŠ” sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    logger.error("pip install faiss-cpu sentence-transformersë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    faiss = None
    SentenceTransformer = None

class PDFProcessor:
    """PDF ì²˜ë¦¬ ë° ë²¡í„°í™” í´ë˜ìŠ¤"""
    
    def __init__(self, vector_db_path: str = "faiss_vector_db"):
        self.vector_db_path = Path(vector_db_path)
        self.vector_db_path.mkdir(exist_ok=True)
        
        # ë¬¸ì œ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.questions_dir = Path("extracted_questions")
        self.questions_dir.mkdir(exist_ok=True)
        
        # ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = None
        self.index = None
        self.documents = []
        self.metadata = []
        
        self._initialize_models()
    
    def _initialize_models(self):
        """ë²¡í„° ëª¨ë¸ ë° FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if SentenceTransformer is None or faiss is None:
            logger.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"ğŸ”§ [INFO] ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
            if device == 'cuda':
                logger.info(f"ğŸ”§ [INFO] GPU ëª¨ë¸: {torch.cuda.get_device_name(0)}")
            
            # í•œêµ­ì–´ì— íŠ¹í™”ëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)
            
            # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (L2 ê±°ë¦¬ ê¸°ë°˜)
            dimension = self.embedding_model.get_sentence_embedding_dimension()
            
            # FAISSëŠ” CPU ì‚¬ìš© (Python 3.12ì—ì„œ GPU FAISS ë¯¸ì§€ì›)
            self.index = faiss.IndexFlatL2(dimension)
            logger.info(f"ğŸ”§ [INFO] FAISS CPU ì¸ë±ìŠ¤ ì‚¬ìš© (Python 3.12)")
            
            logger.info(f"âœ… ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {dimension})")
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def process_pdf(self, pdf_file_path: str, subject: str = "ì •ë³´ì‹œìŠ¤í…œê°ë¦¬ì‚¬", original_filename: str = None) -> Dict[str, Any]:
        """PDF íŒŒì¼ ì²˜ë¦¬ ë° ë²¡í„°í™”"""
        logger.info(f"\nğŸ“„ [PDF ì²˜ë¦¬] íŒŒì¼: {pdf_file_path}")
        
        if DocumentConverter is None:
            return {"success": False, "error": "Docling ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        
        try:
            # Docling GPU ì„¤ì •
            import os
            import torch
            if torch.cuda.is_available():
                os.environ['DOCLING_ACCELERATOR'] = 'cuda'
                logger.info(f"ğŸ”§ [INFO] Docling GPU ì„¤ì •: cuda")
            else:
                os.environ['DOCLING_ACCELERATOR'] = 'cpu'
                logger.info(f"ğŸ”§ [INFO] Docling GPU ì„¤ì •: cpu")
            
            converter = DocumentConverter()
            result = converter.convert(pdf_file_path)
            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Markdown ê¸°ì¤€)
            full_text = result.document.export_to_markdown()
            
            # ë¬¸ì œ ì¶”ì¶œ ë° ì €ì¥
            extracted_questions = self._extract_questions_from_text(full_text, subject, original_filename)
            logger.info(f"ğŸ“ [ë¬¸ì œ ì¶”ì¶œ] {len(extracted_questions)}ê°œì˜ ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ (ìµœëŒ€í•œ ë§ì´)")
            
            # ì¶”ì¶œëœ ë¬¸ì œë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥
            if extracted_questions:
                self._save_questions(extracted_questions, subject, original_filename)
            else:
                logger.warning("âš ï¸ ì¶”ì¶œëœ ë¬¸ì œê°€ ì—†ì–´ì„œ TXT íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ë° ë²¡í„°í™”ëŠ” ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©
            text_chunks = self._extract_and_chunk_text_from_text(full_text, subject)
            if not text_chunks:
                return {"success": False, "error": "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            # ì‹¤ì œ íŒŒì¼ëª… ì‚¬ìš© (ì—†ìœ¼ë©´ ì„ì‹œ íŒŒì¼ëª… ì‚¬ìš©)
            filename_to_use = original_filename if original_filename is not None else str(Path(pdf_file_path).name)
            self._vectorize_and_store(text_chunks, subject, filename_to_use)
            self._save_metadata()
            logger.info(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ - {len(text_chunks)}ê°œ ì²­í¬ ìƒì„±")
            return {
                "success": True,
                "chunks_count": len(text_chunks),
                "questions_count": len(extracted_questions),
                "subject": subject,
                "filename": filename_to_use
            }
        except Exception as e:
            error_msg = f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
            logger.error(f"âŒ {error_msg}")
            return {"success": False, "error": error_msg}
    
    def _extract_questions_from_text(self, full_text: str, subject: str, original_filename: str = None) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œë§Œ ìµœëŒ€í•œ ë§ì´ ì¶”ì¶œ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ì—°ì†ì„± ê²€ì¦)"""
        import re
        lines = full_text.split('\n')
        
        # 3ìë¦¬ê¹Œì§€ ìˆ«ì + ë‹¤ì–‘í•œ êµ¬ë¶„ì íŒ¨í„´ (ì¤„ ì• ê³µë°± í—ˆìš©)
        question_patterns = [
            # ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
            r'^\s*(\d{1,3})\.\s+',  # 108. (ê¸°ë³¸í˜•, ê³µë°± í•„ìˆ˜)
            r'^\s*-\s*(\d{1,3})\.\s+',  # - 108. (ëŒ€ì‹œ ì ‘ë‘ì‚¬, ê³µë°± í•„ìˆ˜)
            r'^\s*(\d{1,3})\)\s+',  # 108) (ê³µë°± í•„ìˆ˜)
            r'^\s*-\s*(\d{1,3})\)\s+',  # - 108) (ëŒ€ì‹œ ì ‘ë‘ì‚¬, ê³µë°± í•„ìˆ˜)
            
            # ë¬¸ì œ ë²ˆí˜¸ë§Œ ìˆëŠ” ê²½ìš°
            r'^\s*(\d{1,3})\.\s*$',  # 108. (ì¤„ ë)
            r'^\s*-\s*(\d{1,3})\.\s*$',  # - 108. (ì¤„ ë)
            
            # ê´„í˜¸í˜•
            r'^\s*\((\d{1,3})\)\s*',  # (108)
            r'^\s*\[(\d{1,3})\]\s*',  # [108]
            r'^\s*ã€(\d{1,3})ã€‘\s*',  # ã€108ã€‘
            r'^\s*ã€ˆ(\d{1,3})ã€‰\s*',  # ã€ˆ108ã€‰
            
            # í‚¤ì›Œë“œí˜•
            r'^\s*ë¬¸ì œ\s*(\d{1,3})\s*[\.\)]?\s*',  # ë¬¸ì œ 108. 
            r'^\s*(\d{1,3})\s*ë²ˆ\s*[\.\)]?\s*',  # 108ë²ˆ.
            r'^\s*ë¬¸í•­\s*(\d{1,3})\s*[\.\)]?\s*',  # ë¬¸í•­ 108.
            
            # íŠ¹ì • í‚¤ì›Œë“œì™€ í•¨ê»˜ ì‹œì‘í•˜ëŠ” íŒ¨í„´ë“¤
            r'^\s*(\d{1,3})\.\s*ë‹¤ìŒ\s*ì¤‘',
            r'^\s*(\d{1,3})\.\s*ì˜¬ë°”ë¥¸\s*ê²ƒì€',
            r'^\s*(\d{1,3})\.\s*í‹€ë¦°\s*ê²ƒì€',
            r'^\s*(\d{1,3})\.\s*ì ì ˆí•œ\s*ê²ƒì€',
            r'^\s*(\d{1,3})\.\s*ê°€ì¥\s*ì ì ˆí•œ',
            
            # ëŒ€ì‹œ ì ‘ë‘ì‚¬ê°€ ìˆëŠ” í‚¤ì›Œë“œ íŒ¨í„´ë“¤
            r'^\s*-\s*(\d{1,3})\.\s*ë‹¤ìŒ\s*ì¤‘',
            r'^\s*-\s*(\d{1,3})\.\s*ì˜¬ë°”ë¥¸\s*ê²ƒì€',
            r'^\s*-\s*(\d{1,3})\.\s*í‹€ë¦°\s*ê²ƒì€',
            r'^\s*-\s*(\d{1,3})\.\s*ì ì ˆí•œ\s*ê²ƒì€',
            r'^\s*-\s*(\d{1,3})\.\s*ê°€ì¥\s*ì ì ˆí•œ',
        ]
        
        # 1ë‹¨ê³„: ëª¨ë“  ê°€ëŠ¥í•œ ë¬¸ì œ ë²ˆí˜¸ ìœ„ì¹˜ ì°¾ê¸°
        potential_questions = []
        logger.info(f"ğŸ” [ë¬¸ì œ ì¶”ì¶œ] ì „ì²´ ë¼ì¸ ìˆ˜: {len(lines)}ê°œ")
        logger.info(f"ğŸ” [ë¬¸ì œ ì¶”ì¶œ] ì²˜ìŒ 10ë¼ì¸ ë¯¸ë¦¬ë³´ê¸°:")
        for i, line in enumerate(lines[:10]):
            logger.info(f"   ë¼ì¸ {i}: {line[:100]}")
        
        for line_idx, line in enumerate(lines):
            line = line.strip('\r')
            if not line:
                continue
            
            # ë””ë²„ê¹…: íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ê°€ í¬í•¨ëœ ë¼ì¸ë“¤ì„ ì§‘ì¤‘ ê²€ì‚¬
            contains_target_numbers = any(f" {i}." in line or f"- {i}." in line or f"{i}." in line[:10] for i in range(50, 80))
            contains_any_number = any(str(i) in line for i in range(1, 1000))
            
            if line_idx < 20 or contains_target_numbers:
                logger.info(f"ğŸ” ë¼ì¸ {line_idx} ê²€ì‚¬: {line}")
                
                # ê° íŒ¨í„´ì„ ê°œë³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
                for pattern_idx, pattern in enumerate(question_patterns):
                    match = re.match(pattern, line)
                    if match:
                        detected_number = match.group(1)
                        logger.info(f"   âœ… íŒ¨í„´ {pattern_idx} ë§¤ì¹­ ì„±ê³µ: {pattern}")
                        logger.info(f"   ğŸ“ ì¶”ì¶œëœ ë²ˆí˜¸: {detected_number}")
                    elif contains_target_numbers:
                        logger.info(f"   âŒ íŒ¨í„´ {pattern_idx} ë§¤ì¹­ ì‹¤íŒ¨: {pattern}")
                
            for pattern_idx, pattern in enumerate(question_patterns):
                match = re.match(pattern, line)
                if match:
                    detected_number = match.group(1)
                    if detected_number.isdigit() and 1 <= int(detected_number) <= 999:
                        potential_questions.append({
                            "line_idx": line_idx,
                            "number": int(detected_number),
                            "line": line
                        })
                        logger.info(f"âœ… ë¬¸ì œ ë°œê²¬: {detected_number}ë²ˆ (ë¼ì¸ {line_idx}, íŒ¨í„´ {pattern_idx}: {pattern})")
                        logger.info(f"   ì „ì²´ ë¼ì¸: {line}")
                        break
        
        logger.info(f"ğŸ” [1ë‹¨ê³„] ì ì¬ ë¬¸ì œ {len(potential_questions)}ê°œ ë°œê²¬")
        if potential_questions:
            logger.info(f"ğŸ“‹ ë°œê²¬ëœ ë¬¸ì œ ë²ˆí˜¸ë“¤: {[q['number'] for q in potential_questions]}")
        
        # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±° ë° ë²ˆí˜¸ìˆœ ì •ë ¬
        # ê°™ì€ ë²ˆí˜¸ì˜ ì¤‘ë³µ ë¬¸ì œ ì œê±° (ê°€ì¥ ë¨¼ì € ë°œê²¬ëœ ê²ƒë§Œ ìœ ì§€)
        seen_numbers = set()
        verified_questions = []
        
        for potential_q in potential_questions:
            if potential_q["number"] not in seen_numbers:
                verified_questions.append(potential_q)
                seen_numbers.add(potential_q["number"])
                logger.debug(f"âœ… ë¬¸ì œ {potential_q['number']}ë²ˆ ì¶”ê°€")
            else:
                logger.debug(f"âŒ ë¬¸ì œ {potential_q['number']}ë²ˆ ì¤‘ë³µ ì œì™¸")
        
        # ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        verified_questions.sort(key=lambda x: x["number"])
        
        logger.info(f"ğŸ” [2ë‹¨ê³„] ì—°ì†ì„± ê²€ì¦ í›„ ë¬¸ì œ {len(verified_questions)}ê°œ í™•ì •")
        
        # 3ë‹¨ê³„: ê²€ì¦ëœ ë¬¸ì œë“¤ ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        questions = []
        for i, verified_q in enumerate(verified_questions):
            start_line_idx = verified_q["line_idx"]
            
            # ë‹¤ìŒ ë¬¸ì œê¹Œì§€ì˜ ë¼ì¸ ë²”ìœ„ ê²°ì •
            if i + 1 < len(verified_questions):
                end_line_idx = verified_questions[i + 1]["line_idx"] - 1
            else:
                end_line_idx = len(lines) - 1
            
            # ë¬¸ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            question_lines = []
            for line_idx in range(start_line_idx, min(end_line_idx + 1, len(lines))):
                line = lines[line_idx].strip('\r')
                if line:  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                    question_lines.append(line)
            
            if question_lines:
                question_text = '\n'.join(question_lines)
                questions.append({
                    "number": str(verified_q["number"]),
                    "text": question_text,
                    "start_line": start_line_idx,
                    "end_line": end_line_idx
                })
                logger.debug(f"ğŸ“ ë¬¸ì œ {verified_q['number']}ë²ˆ ì¶”ì¶œ: {len(question_text)} ë¬¸ì")
        
        logger.info(f"ğŸ“ [3ë‹¨ê³„] ìµœì¢… ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ: {len(questions)}ê°œ")
        logger.info(f"ğŸ“‹ [ì¶”ì¶œëœ ë¬¸ì œ ë²ˆí˜¸]: {[q['number'] for q in questions]}")
        
        return questions
    
    def _extract_questions_with_ai(self, full_text: str, subject: str) -> List[Dict[str, Any]]:
        """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œ ì¶”ì¶œ (ì •ê·œí‘œí˜„ì‹ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²•)"""
        try:
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            text_sample = full_text[:10000] if len(full_text) > 10000 else full_text
            
            # ë¬¸ì œ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            prompt = f"""
ë‹¤ìŒ {subject} ê¸°ì¶œë¬¸ì œ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text_sample}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

=== ë¬¸ì œ 1 ===
[ë¬¸ì œ ë‚´ìš©]

=== ë¬¸ì œ 2 ===
[ë¬¸ì œ ë‚´ìš©]

...

ë¬¸ì œ ë²ˆí˜¸ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ìˆœì„œëŒ€ë¡œ 1, 2, 3...ìœ¼ë¡œ ë¶€ì—¬í•´ì£¼ì„¸ìš”.
ë¬¸ì œëŠ” ê°ê´€ì‹, ì£¼ê´€ì‹, ì„œìˆ í˜• ë“± ëª¨ë“  ìœ í˜•ì„ í¬í•¨í•©ë‹ˆë‹¤.
ë¬¸ì œì˜ ì‹œì‘ê³¼ ëì„ ëª…í™•íˆ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
"""

            # Azure OpenAI API í˜¸ì¶œ
            import openai
            from config import Config
            
            # OpenAI ì„¤ì •
            openai.api_key = Config.OPENAI_API_KEY
            openai.azure_endpoint = Config.AZURE_ENDPOINT
            openai.api_type = Config.OPENAI_API_TYPE
            openai.api_version = Config.OPENAI_API_VERSION
            
            response = openai.chat.completions.create(
                model=Config.DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸°ì¶œë¬¸ì œ í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
            )
            
            result = response.choices[0].message.content
            if not result:
                return []
            
            # AI ì‘ë‹µì—ì„œ ë¬¸ì œ íŒŒì‹±
            questions = []
            current_question = None
            current_lines = []
            
            lines = result.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ë¬¸ì œ ì‹œì‘ ê°ì§€
                if line.startswith('=== ë¬¸ì œ ') and line.endswith(' ==='):
                    # ì´ì „ ë¬¸ì œ ì €ì¥
                    if current_question and current_lines:
                        question_text = '\n'.join(current_lines).strip()
                        if len(question_text) > 30:
                            questions.append({
                                "number": current_question,
                                "text": question_text,
                                "start_line": 0,
                                "end_line": 0
                            })
                    
                    # ìƒˆ ë¬¸ì œ ì‹œì‘
                    current_question = line.replace('=== ë¬¸ì œ ', '').replace(' ===', '')
                    current_lines = []
                else:
                    # í˜„ì¬ ë¬¸ì œì— ë¼ì¸ ì¶”ê°€
                    if current_question is not None:
                        current_lines.append(line)
            
            # ë§ˆì§€ë§‰ ë¬¸ì œ ì €ì¥
            if current_question and current_lines:
                question_text = '\n'.join(current_lines).strip()
                if len(question_text) > 30:
                    questions.append({
                        "number": current_question,
                        "text": question_text,
                        "start_line": 0,
                        "end_line": 0
                    })
            
            logger.info(f"ğŸ¤– [AI ë¬¸ì œ ì¶”ì¶œ] {len(questions)}ê°œ ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ")
            return questions
            
        except Exception as e:
            logger.error(f"âŒ [AI ë¬¸ì œ ì¶”ì¶œ] ì˜¤ë¥˜: {e}")
            return []
    
    def _save_questions(self, questions: List[Dict[str, Any]], subject: str, original_filename: str = None):
        """ì¶”ì¶œëœ ë¬¸ì œë¥¼ txt íŒŒì¼ë¡œë§Œ ì €ì¥"""
        try:
            logger.info(f"ğŸ’¾ [ë¬¸ì œ ì €ì¥] {len(questions)}ê°œ ë¬¸ì œ ì €ì¥ ì‹œì‘...")
            
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            base_filename = (original_filename or "").replace('.pdf', '') if original_filename else f"{subject}_{timestamp}"
            
            # TXT íŒŒì¼ë§Œ ì €ì¥
            txt_file = self.questions_dir / f"{base_filename}_questions.txt"
            logger.info(f"ğŸ“„ [ë¬¸ì œ ì €ì¥] ì €ì¥í•  íŒŒì¼: {txt_file}")
            
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(f"# {subject} ê¸°ì¶œë¬¸ì œ\n")
                f.write(f"# ì¶œì²˜: {original_filename}\n")
                f.write(f"# ì¶”ì¶œì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"# ì´ ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ\n\n")
                
                for i, question in enumerate(questions, 1):
                    f.write(f"=== ë¬¸ì œ {question['number']} ===\n")
                    f.write(f"{question['text']}\n\n")
            
            logger.info(f"âœ… ë¬¸ì œ ì €ì¥ ì™„ë£Œ:")
            logger.info(f"   ğŸ“„ TXT: {txt_file}")
            logger.info(f"   ğŸ“Š ì €ì¥ëœ ë¬¸ì œ ìˆ˜: {len(questions)}ê°œ")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    
    def _extract_and_chunk_text_from_text(self, full_text: str, subject: str) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸(ë¬¸ìì—´)ì—ì„œ ì²­í¬ ë¶„í• """
        chunks = []
        try:
            # í‘œ ê°ì§€ ë° íŠ¹ë³„ ì²˜ë¦¬
            lines = full_text.split('\n')
            current_chunk_lines = []
            current_length = 0
            in_table = False
            table_start_line = -1
            
            for i, line in enumerate(lines):
                # í‘œ ì‹œì‘/ë ê°ì§€
                is_table_line = line.strip().startswith('|') or ('|' in line.strip() and len(line.strip()) > 10)
                
                if is_table_line and not in_table:
                    # í‘œ ì‹œì‘
                    in_table = True
                    table_start_line = i
                    # í‘œ ì‹œì‘ ì „ê¹Œì§€ì˜ ì²­í¬ ì €ì¥
                    if current_chunk_lines and len('\n'.join(current_chunk_lines).strip()) >= 100:
                        chunk_text = '\n'.join(current_chunk_lines)
                        chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_{chunk_text[:50]}".encode()).hexdigest()
                        chunks.append({
                            "id": chunk_id,
                            "text": chunk_text.strip(),
                            "start_pos": len(chunks) * 1000,
                            "end_pos": len(chunks) * 1000 + len(chunk_text),
                            "subject": subject,
                            "created_at": datetime.now().isoformat()
                        })
                    current_chunk_lines = []
                    current_length = 0
                
                elif not is_table_line and in_table:
                    # í‘œ ë
                    in_table = False
                    # í‘œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì €ì¥
                    table_lines = lines[table_start_line:i]
                    table_text = '\n'.join(table_lines)
                    if len(table_text.strip()) >= 50:
                        chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_table_{table_start_line}".encode()).hexdigest()
                        chunks.append({
                            "id": chunk_id,
                            "text": table_text.strip(),
                            "start_pos": len(chunks) * 1000,
                            "end_pos": len(chunks) * 1000 + len(table_text),
                            "subject": subject,
                            "created_at": datetime.now().isoformat(),
                            "is_table": True
                        })
                    current_chunk_lines = []
                    current_length = 0
                
                # í˜„ì¬ ë¼ì¸ ì¶”ê°€
                if not in_table:
                    current_chunk_lines.append(line)
                    current_length += len(line) + 1
                    
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í¬ í¬ê¸° ì²´í¬
                    if current_length >= 1000:
                        chunk_text = '\n'.join(current_chunk_lines)
                        if len(chunk_text.strip()) >= 100:
                            chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_{chunk_text[:50]}".encode()).hexdigest()
                            chunks.append({
                                "id": chunk_id,
                                "text": chunk_text.strip(),
                                "start_pos": len(chunks) * 1000,
                                "end_pos": len(chunks) * 1000 + len(chunk_text),
                                "subject": subject,
                                "created_at": datetime.now().isoformat()
                            })
                        current_chunk_lines = []
                        current_length = 0
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
            if current_chunk_lines:
                chunk_text = '\n'.join(current_chunk_lines)
                if len(chunk_text.strip()) >= 100:
                    chunk_id = hashlib.md5(f"{subject}_{len(chunks)}_{chunk_text[:50]}".encode()).hexdigest()
                    chunks.append({
                        "id": chunk_id,
                        "text": chunk_text.strip(),
                        "start_pos": len(chunks) * 1000,
                        "end_pos": len(chunks) * 1000 + len(chunk_text),
                        "subject": subject,
                        "created_at": datetime.now().isoformat()
                    })
                
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return chunks
    
    def _vectorize_and_store(self, chunks: List[Dict[str, Any]], subject: str, pdf_file_path: str):
        """ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ê³  FAISSì— ì €ì¥"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            texts = [chunk["text"] for chunk in chunks]
            
            # ë²¡í„°í™”
            logger.info("ğŸ”„ í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")
            embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embeddings.astype('float32'))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            for i, chunk in enumerate(chunks):
                chunk["embedding_id"] = len(self.documents) + i
                chunk["pdf_source"] = str(Path(pdf_file_path).name)
                self.documents.append(chunk["text"])
                self.metadata.append(chunk)
            
            logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ë²¡í„°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë²¡í„°í™” ì¤‘ ì˜¤ë¥˜: {e}")
    
    def search_similar_chunks(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰"""
        if self.embedding_model is None or self.index is None:
            return []
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.embedding_model.encode([query])
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(query_embedding.astype('float32'), n_results)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(self.metadata):
                    result = {
                        "rank": i + 1,
                        "distance": float(distance),
                        "text": self.documents[idx],
                        "metadata": self.metadata[idx]
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def get_chunks_by_subject(self, subject: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ê³¼ëª©ë³„ ì²­í¬ ì¡°íšŒ"""
        results = []
        
        for metadata in self.metadata:
            if metadata.get("subject") == subject:
                results.append({
                    "text": metadata.get("text", ""),
                    "metadata": metadata
                })
                if len(results) >= n_results:
                    break
        
        return results
    
    def _save_metadata(self):
        """ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            metadata_file = self.vector_db_path / "metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_chunks": len(self.documents),
                    "metadata": self.metadata,
                    "last_updated": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            if self.index:
                index_file = self.vector_db_path / "faiss_index.bin"
                faiss.write_index(self.index, str(index_file))
            
            logger.info(f"âœ… ë©”íƒ€ë°ì´í„° ë° ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def load_existing_data(self):
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        try:
            metadata_file = self.vector_db_path / "metadata.json"
            index_file = self.vector_db_path / "faiss_index.bin"
            
            if metadata_file.exists() and index_file.exists():
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.metadata = data.get("metadata", [])
                    self.documents = [meta.get("text", "") for meta in self.metadata]
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                if faiss:
                    self.index = faiss.read_index(str(index_file))
                
                logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - {len(self.documents)}ê°œ ì²­í¬")
                return True
            
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë²¡í„° DB í†µê³„ ì •ë³´"""
        stats = {
            "total_chunks": len(self.documents),
            "total_metadata": len(self.metadata),
            "index_size": self.index.ntotal if self.index else 0,
            "subjects": list(set([meta.get("subject", "") for meta in self.metadata if meta.get("subject")])),
            "pdf_sources": list(set([meta.get("pdf_source", "") for meta in self.metadata if meta.get("pdf_source")]))
        }
        return stats
    
    def clear_all_data(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        try:
            self.documents = []
            self.metadata = []
            if self.index:
                self.index.reset()
            
            # íŒŒì¼ ì‚­ì œ
            metadata_file = self.vector_db_path / "metadata.json"
            index_file = self.vector_db_path / "faiss_index.bin"
            
            if metadata_file.exists():
                metadata_file.unlink()
            if index_file.exists():
                index_file.unlink()
            
            logger.info("âœ… ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_extracted_questions(self, subject: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì‹œí—˜ì˜ ì¶”ì¶œëœ ë¬¸ì œ ëª©ë¡ ì¡°íšŒ (TXT íŒŒì¼ë§Œ ì‚¬ìš©)"""
        questions = []
        
        try:
            # ëª¨ë“  TXT íŒŒì¼ ê²€ìƒ‰ (ë” ìœ ì—°í•œ ë§¤ì¹­)
            all_txt_files = list(self.questions_dir.glob("*_questions.txt"))
            logger.info(f"ğŸ” [íŒŒì¼ ê²€ìƒ‰] ì „ì²´ TXT íŒŒì¼: {[f.name for f in all_txt_files]}")
            
            # ê³¼ëª©ëª… ë§¤ì¹­ (ê³µë°± ì œê±°í•˜ì—¬ ë¹„êµ)
            subject_clean = subject.replace(" ", "").replace("ã€€", "")  # ê³µë°±ê³¼ ì „ê°ê³µë°± ì œê±°
            matching_files = []
            
            for txt_file in all_txt_files:
                filename_clean = txt_file.name.replace(" ", "").replace("ã€€", "")  # ê³µë°± ì œê±°
                if subject_clean in filename_clean or subject in txt_file.name:
                    matching_files.append(txt_file)
                    logger.info(f"âœ… [íŒŒì¼ ë§¤ì¹­] {txt_file.name} - ë§¤ì¹­ë¨")
                else:
                    logger.debug(f"âŒ [íŒŒì¼ ë§¤ì¹­] {txt_file.name} - ë§¤ì¹­ ì•ˆë¨")
            
            logger.info(f"ğŸ” [íŒŒì¼ ê²€ìƒ‰] {subject}ì™€ ë§¤ì¹­ëœ íŒŒì¼: {len(matching_files)}ê°œ")
            
            for txt_file in matching_files:
                try:
                    txt_questions = self._parse_questions_from_txt(txt_file, subject)
                    questions.extend(txt_questions)
                    logger.info(f"ğŸ“„ [íŒŒì¼ ë¡œë“œ] {txt_file.name}ì—ì„œ {len(txt_questions)}ê°œ ë¬¸ì œ ë¡œë“œ")
                except Exception as e:
                    logger.warning(f"âš ï¸ TXT íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({txt_file}): {e}")
                    continue
            
            # ë¬¸ì œ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            questions.sort(key=lambda x: int(x.get("number", 0)) if x.get("number", "0").isdigit() else 0)
            
            logger.info(f"âœ… {subject} ì‹œí—˜ì˜ ì¶”ì¶œëœ ë¬¸ì œ {len(questions)}ê°œ ë¡œë“œ ì™„ë£Œ (TXT íŒŒì¼ë§Œ ì‚¬ìš©)")
            return questions
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œëœ ë¬¸ì œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _parse_questions_from_txt(self, txt_file: Path, subject: str) -> List[Dict[str, Any]]:
        """TXT íŒŒì¼ì—ì„œ ë¬¸ì œë“¤ì„ íŒŒì‹± (ê°œë³„ ë¬¸ì œ ë¶„ë¦¬)"""
        questions = []
        
        try:
            logger.info(f"ğŸ” [TXT íŒŒì‹±] íŒŒì¼ ì½ê¸° ì‹œì‘: {txt_file.name}")
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            logger.info(f"ğŸ” [TXT íŒŒì‹±] íŒŒì¼ í¬ê¸°: {len(content)} ë¬¸ì")
            logger.info(f"ğŸ” [TXT íŒŒì‹±] íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 500ì):")
            logger.info(content[:500])
            
            # íŒŒì¼ì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
            source_file = "unknown"
            extraction_date = ""
            
            lines = content.split('\n')
            logger.info(f"ğŸ” [TXT íŒŒì‹±] ì „ì²´ ë¼ì¸ ìˆ˜: {len(lines)}ê°œ")
            
            for line in lines:
                if line.startswith('# ì¶œì²˜:'):
                    source_file = line.replace('# ì¶œì²˜:', '').strip()
                    logger.info(f"ğŸ“„ [TXT íŒŒì‹±] ì¶œì²˜ ì •ë³´: {source_file}")
                elif line.startswith('# ì¶”ì¶œì¼:'):
                    extraction_date = line.replace('# ì¶”ì¶œì¼:', '').strip()
                    logger.info(f"ğŸ“… [TXT íŒŒì‹±] ì¶”ì¶œì¼: {extraction_date}")
            
            # ë¬¸ì œ ì„¹ì…˜ ë¶„ë¦¬
            question_sections = content.split('=== ë¬¸ì œ ')
            logger.info(f"ğŸ” [TXT íŒŒì‹±] '=== ë¬¸ì œ 'ë¡œ ë¶„ë¦¬í•œ ì„¹ì…˜ ìˆ˜: {len(question_sections)}ê°œ")
            
            for i, section in enumerate(question_sections):
                logger.info(f"ğŸ” [TXT íŒŒì‹±] ì„¹ì…˜ {i}: {len(section)} ë¬¸ì")
                if i > 0:  # ì²« ë²ˆì§¸ ì„¹ì…˜ì€ í—¤ë”
                    logger.info(f"   ì„¹ì…˜ {i} ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {section[:100]}...")
            
            for section_idx, section in enumerate(question_sections[1:], 1):  # ì²« ë²ˆì§¸ëŠ” í—¤ë”ì´ë¯€ë¡œ ì œì™¸
                logger.info(f"ğŸ” [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx} ì²˜ë¦¬ ì¤‘...")
                lines = section.split('\n')
                if not lines:
                    logger.warning(f"âš ï¸ [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx}: ë¹ˆ ì„¹ì…˜")
                    continue
                
                # ë¬¸ì œ ë²ˆí˜¸ ì¶”ì¶œ
                first_line = lines[0].strip()
                logger.info(f"ğŸ” [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx} ì²« ë¼ì¸: '{first_line}'")
                
                if not first_line:
                    logger.warning(f"âš ï¸ [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx}: ì²« ë¼ì¸ì´ ë¹„ì–´ìˆìŒ")
                    continue
                    
                # === ì œê±° í›„ ìˆ«ì í™•ì¸
                clean_line = first_line.replace('=', '').strip()
                logger.info(f"ğŸ” [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx} === ì œê±° í›„: '{clean_line}'")
                
                if not clean_line.isdigit():
                    logger.warning(f"âš ï¸ [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx}: ë¬¸ì œ ë²ˆí˜¸ê°€ ìˆ«ìê°€ ì•„ë‹˜: '{clean_line}'")
                    continue
                
                section_number = clean_line
                logger.info(f"âœ… [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx}: ë¬¸ì œ ë²ˆí˜¸ '{section_number}' ì¶”ì¶œ")
                
                # ë¬¸ì œ ë‚´ìš© ì¶”ì¶œ (ë¬¸ì œ ë²ˆí˜¸ ë¼ì¸ ì œì™¸)
                section_content = '\n'.join(lines[1:]).strip()
                logger.info(f"ğŸ” [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx}: ë‚´ìš© ê¸¸ì´ {len(section_content)} ë¬¸ì")
                
                # TXT íŒŒì¼ì—ì„œëŠ” ê° ì„¹ì…˜ì´ ì´ë¯¸ ì™„ì „í•œ ë¬¸ì œì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                if section_content:
                    questions.append({
                        "number": section_number,
                        "text": section_content,
                        "source_file": source_file,
                        "extraction_date": extraction_date,
                        "start_line": 0,
                        "end_line": 0
                    })
                    logger.info(f"âœ… [TXT íŒŒì‹±] ë¬¸ì œ {section_number}ë²ˆ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œ: {len(section_content)} ë¬¸ì")
                else:
                    logger.warning(f"âš ï¸ [TXT íŒŒì‹±] ì„¹ì…˜ {section_idx}: ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")
            
            logger.info(f"ğŸ“„ TXT íŒŒì¼ì—ì„œ {len(questions)}ê°œ ë¬¸ì œ íŒŒì‹± ì™„ë£Œ: {txt_file.name}")
            return questions
            
        except Exception as e:
            logger.error(f"âŒ TXT íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨ {txt_file}: {e}")
            import traceback
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return []
    
    def _extract_individual_questions_from_section(self, section_content: str, section_number: str, 
                                                 source_file: str, extraction_date: str) -> List[Dict[str, Any]]:
        """ì„¹ì…˜ ë‚´ìš©ì—ì„œ ê°œë³„ ë¬¸ì œë“¤ì„ ì¶”ì¶œ (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        questions = []
        
        # ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ë“¤ (ë” ì •í™•í•œ íŒ¨í„´)
        question_patterns = [
            r'^(\d+)\.\s*',  # 38. (ê³µë°± í¬í•¨)
        ]
        
        lines = section_content.split('\n')
        current_question = None
        current_question_lines = []
        current_question_number = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ë¬¸ì œ ì‹œì‘ ê°ì§€
            is_question_start = False
            detected_number = None
            
            for pattern in question_patterns:
                import re
                match = re.match(pattern, line)
                if match:
                    detected_number = match.group(1)
                    # ì—°ë„ê°€ ì•„ë‹Œ ë¬¸ì œ ë²ˆí˜¸ì¸ì§€ í™•ì¸ (1-1000 ë²”ìœ„ë¡œ í™•ì¥)
                    if detected_number.isdigit() and 1 <= int(detected_number) <= 1000:
                        is_question_start = True
                        logger.debug(f"âœ… ë¬¸ì œ ì‹œì‘ ê°ì§€: {line[:50]}... (íŒ¨í„´: {pattern})")
                        break
            
            if not is_question_start and line.startswith(('38.', '39.', '40.', '41.', '42.')):
                logger.debug(f"ğŸ” íŒ¨í„´ ë¯¸ë§¤ì¹­ ë¼ì¸: {line[:50]}...")
            
            if is_question_start:
                # ì´ì „ ë¬¸ì œ ì €ì¥
                if current_question and current_question_lines:
                    question_text = '\n'.join(current_question_lines).strip()
                    if len(question_text) > 20:  # ìµœì†Œ ê¸¸ì´ ì²´í¬ (ì™„í™”)
                        questions.append({
                            "number": current_question_number,
                            "text": question_text,
                            "source_file": source_file,
                            "extraction_date": extraction_date,
                            "start_line": 0,
                            "end_line": 0
                        })
                        logger.debug(f"âœ… ë¬¸ì œ {current_question_number} ì¶”ì¶œ: {len(question_text)} ë¬¸ì")
                
                # ìƒˆ ë¬¸ì œ ì‹œì‘
                current_question = True
                current_question_lines = [line]
                current_question_number = detected_number
            else:
                # í˜„ì¬ ë¬¸ì œì— ë¼ì¸ ì¶”ê°€
                if current_question is not None:
                    current_question_lines.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì œ ì €ì¥
        if current_question and current_question_lines:
            question_text = '\n'.join(current_question_lines).strip()
            if len(question_text) > 20:
                questions.append({
                    "number": current_question_number,
                    "text": question_text,
                    "source_file": source_file,
                    "extraction_date": extraction_date,
                    "start_line": 0,
                    "end_line": 0
                })
                logger.debug(f"âœ… ë§ˆì§€ë§‰ ë¬¸ì œ {current_question_number} ì¶”ì¶œ: {len(question_text)} ë¬¸ì")
        
        return questions
    
    def search_extracted_questions(self, query: str, subject: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ì¶”ì¶œëœ ë¬¸ì œì—ì„œ ê²€ìƒ‰"""
        questions = self.get_extracted_questions(subject)
        if not questions:
            return []
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
        query_lower = query.lower()
        matched_questions = []
        
        for question in questions:
            question_text = question.get("text", "").lower()
            if query_lower in question_text:
                matched_questions.append(question)
        
        # ìƒìœ„ n_resultsê°œ ë°˜í™˜
        return matched_questions[:n_results]
    
    def get_random_extracted_question(self, subject: str) -> Optional[Dict[str, Any]]:
        """ì¶”ì¶œëœ ë¬¸ì œì—ì„œ ëœë¤ ì„ íƒ"""
        questions = self.get_extracted_questions(subject)
        if not questions:
            return None
        
        import random
        return random.choice(questions)
    
    def get_extracted_question_by_number(self, subject: str, question_number: str) -> Optional[Dict[str, Any]]:
        """ë¬¸ì œ ë²ˆí˜¸ë¡œ íŠ¹ì • ë¬¸ì œ ì¡°íšŒ"""
        questions = self.get_extracted_questions(subject)
        
        for question in questions:
            if question.get("number") == question_number:
                return question
        
        return None
    
    def search_extracted_questions_semantic(self, query: str, subject: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ì¶”ì¶œëœ ë¬¸ì œì—ì„œ semantic ê²€ìƒ‰"""
        questions = self.get_extracted_questions(subject)
        if not questions:
            return []
        
        if self.embedding_model is None:
            logger.warning("âš ï¸ ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•„ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self.search_extracted_questions(query, subject, n_results)
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.embedding_model.encode([query])
            
            # ëª¨ë“  ë¬¸ì œ í…ìŠ¤íŠ¸ ë²¡í„°í™”
            question_texts = [q["text"] for q in questions]
            question_embeddings = self.embedding_model.encode(question_texts)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, question_embeddings)[0]
            
            # ìœ ì‚¬ë„ì™€ í•¨ê»˜ ê²°ê³¼ êµ¬ì„±
            results_with_scores = []
            for i, (question, similarity) in enumerate(zip(questions, similarities)):
                results_with_scores.append({
                    "question": question,
                    "score": float(similarity),
                    "rank": i + 1
                })
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            results_with_scores.sort(key=lambda x: x["score"], reverse=True)
            
            # ìƒìœ„ n_resultsê°œ ë°˜í™˜
            top_results = results_with_scores[:n_results]
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for result in top_results:
                question_data = result["question"]
                results.append({
                    "content": question_data["text"],
                    "metadata": {
                        "type": "extracted_question",
                        "subject": subject,
                        "question_number": question_data["number"],
                        "pdf_source": "ì¶”ì¶œëœ ê¸°ì¶œë¬¸ì œ",
                        "score": result["score"],
                        "rank": result["rank"]
                    },
                    "score": result["score"]
                })
            
            logger.info(f"âœ… ì¶”ì¶œëœ ë¬¸ì œ semantic ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ì¶”ì¶œëœ ë¬¸ì œ semantic ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´
            return self.search_extracted_questions(query, subject, n_results)

# ì „ì—­ PDF í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
pdf_processor = PDFProcessor() 