"""
ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ ì‹œìŠ¤í…œ
FAISSë¥¼ ì‚¬ìš©í•œ ë¬¸ì„œ ì €ì¥ ë° ê²€ìƒ‰
"""

import os
from typing import List, Dict, Any, Optional
import json
from pathlib import Path
import hashlib
from datetime import datetime
import numpy as np
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# FAISS ê´€ë ¨ import
try:
    import faiss
    from sentence_transformers import SentenceTransformer
except ImportError:
    logger.error("FAISS ë˜ëŠ” sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    logger.error("pip install faiss-cpu sentence-transformersë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    faiss = None
    SentenceTransformer = None

class VectorStore:
    """FAISS ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, persist_directory: str = "faiss_vector_db"):
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(exist_ok=True)
        
        # ë²¡í„° ëª¨ë¸ ë° ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.embedding_model = None
        self.index = None
        self.documents = []
        self.metadata = []
        
        self._initialize_models()
        self._load_existing_data()
    
    def _initialize_models(self):
        """ë²¡í„° ëª¨ë¸ ë° FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        if SentenceTransformer is None or faiss is None:
            logger.error("í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"ğŸ”§ [INFO] ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤: {device}")
            if device == 'cuda':
                logger.info(f"ğŸ”§ [INFO] GPU ëª¨ë¸: {torch.cuda.get_device_name(0)}")
            
            # í•œêµ­ì–´ì— íŠ¹í™”ëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)
            
            # FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™” (L2 ê±°ë¦¬ ê¸°ë°˜)
            dimension = self.embedding_model.get_sentence_embedding_dimension()
            
            # FAISSëŠ” CPU ì‚¬ìš© (Python 3.12ì—ì„œ GPU FAISS ë¯¸ì§€ì›)
            self.index = faiss.IndexFlatL2(dimension)
            logger.info(f"ğŸ”§ [INFO] FAISS CPU ì¸ë±ìŠ¤ ì‚¬ìš© (Python 3.12)")
            
            logger.info(f"âœ… FAISS ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {dimension})")
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _load_existing_data(self):
        """ê¸°ì¡´ ë°ì´í„° ë¡œë“œ"""
        try:
            metadata_file = self.persist_directory / "metadata.json"
            index_file = self.persist_directory / "faiss_index.bin"
            
            if metadata_file.exists() and index_file.exists():
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.metadata = data.get("metadata", [])
                    self.documents = [meta.get("text", "") for meta in self.metadata]
                
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(str(index_file))
                
                logger.info(f"âœ… ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ - {len(self.documents)}ê°œ ë¬¸ì„œ")
                return True
            
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    def add_exam_question(self, question_data: Dict[str, Any]):
        """ì‹œí—˜ ë¬¸ì œ ì¶”ê°€"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # ë¬¸ì„œ ID ìƒì„±
        doc_id = hashlib.md5(
            f"{question_data.get('subject', '')}{question_data.get('question', '')}".encode()
        ).hexdigest()
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = {
            "id": doc_id,
            "type": "exam_question",
            "subject": question_data.get("subject", ""),
            "difficulty": question_data.get("difficulty", ""),
            "question_type": question_data.get("question_type", ""),
            "correct_answer": question_data.get("correct_answer", ""),
            "explanation": question_data.get("explanation", ""),
            "source": question_data.get("source", ""),
            "created_at": datetime.now().isoformat()
        }
        
        # ë¬¸ì„œ ë‚´ìš© (ê²€ìƒ‰ìš©)
        document_content = f"""
        ê³¼ëª©: {question_data.get('subject', '')}
        ë¬¸ì œ: {question_data.get('question', '')}
        ë³´ê¸°: {question_data.get('options', '')}
        ì •ë‹µ: {question_data.get('correct_answer', '')}
        í•´ì„¤: {question_data.get('explanation', '')}
        """
        
        try:
            # ë²¡í„°í™”
            embedding = self.embedding_model.encode([document_content])
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embedding.astype('float32'))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata["embedding_id"] = len(self.documents)
            self.documents.append(document_content)
            self.metadata.append(metadata)
            
            # ì €ì¥
            self._save_data()
            
            logger.info(f"âœ… ì‹œí—˜ ë¬¸ì œ ì¶”ê°€ ì™„ë£Œ: {doc_id}")
            return doc_id
            
        except Exception as e:
            logger.error(f"ë¬¸ì œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def add_study_material(self, material_data: Dict[str, Any]):
        """í•™ìŠµ ìë£Œ ì¶”ê°€"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        doc_id = hashlib.md5(
            f"{material_data.get('title', '')}{material_data.get('content', '')}".encode()
        ).hexdigest()
        
        metadata = {
            "id": doc_id,
            "type": "study_material",
            "title": material_data.get("title", ""),
            "category": material_data.get("category", ""),
            "subject": material_data.get("subject", ""),
            "difficulty": material_data.get("difficulty", ""),
            "source": material_data.get("source", ""),
            "created_at": datetime.now().isoformat()
        }
        
        try:
            # ë²¡í„°í™”
            embedding = self.embedding_model.encode([material_data.get("content", "")])
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embedding.astype('float32'))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata["embedding_id"] = len(self.documents)
            self.documents.append(material_data.get("content", ""))
            self.metadata.append(metadata)
            
            # ì €ì¥
            self._save_data()
            
            logger.info(f"âœ… í•™ìŠµ ìë£Œ ì¶”ê°€ ì™„ë£Œ: {doc_id}")
            return doc_id
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ìë£Œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def add_user_question(self, user_id: str, question_data: Dict[str, Any]):
        """ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        doc_id = hashlib.md5(
            f"{user_id}{question_data.get('question', '')}{datetime.now().isoformat()}".encode()
        ).hexdigest()
        
        metadata = {
            "id": doc_id,
            "type": "user_question",
            "user_id": user_id,
            "subject": question_data.get("subject", ""),
            "difficulty": question_data.get("difficulty", ""),
            "created_at": datetime.now().isoformat()
        }
        
        try:
            # ë²¡í„°í™”
            embedding = self.embedding_model.encode([question_data.get("question", "")])
            
            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self.index.add(embedding.astype('float32'))
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata["embedding_id"] = len(self.documents)
            self.documents.append(question_data.get("question", ""))
            self.metadata.append(metadata)
            
            # ì €ì¥
            self._save_data()
            
            logger.info(f"âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€ ì™„ë£Œ: {doc_id}")
            return doc_id
            
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def search_similar_questions(self, query: str, subject: Optional[str] = None, 
                               n_results: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ë¬¸ì œ ê²€ìƒ‰"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        if len(self.documents) == 0:
            logger.error("ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.embedding_model.encode([query])
            
            # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ ì œí•œ (ì¸ë±ìŠ¤ í¬ê¸°ë³´ë‹¤ í¬ë©´ ì•ˆë¨)
            search_k = min(n_results * 2, len(self.documents))
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(query_embedding.astype('float32'), search_k)
            
            # ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if len(distances) == 0 or len(indices) == 0:
                logger.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê²°ê³¼ í•„í„°ë§ ë° í¬ë§·íŒ…
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
                if idx < 0 or idx >= len(self.metadata):
                    continue
                    
                metadata = self.metadata[idx]
                
                # ê³¼ëª© í•„í„°ë§
                if subject and metadata.get("subject") != subject:
                    continue
                
                # íƒ€ì… í•„í„°ë§ (typeì´ ì—†ìœ¼ë©´ ëª¨ë“  ë¬¸ì„œ í—ˆìš©)
                if metadata.get("type") and metadata.get("type") != "exam_question":
                    continue
                
                result = {
                    "id": metadata.get("id"),
                    "content": self.documents[idx],
                    "metadata": metadata,
                    "distance": float(distance),
                    "rank": len(results) + 1
                }
                results.append(result)
                
                if len(results) >= n_results:
                    break
            
            return results
            
        except Exception as e:
            logger.error(f"ë¬¸ì œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def search_study_materials(self, query: str, subject: Optional[str] = None,
                             n_results: int = 5) -> List[Dict[str, Any]]:
        """í•™ìŠµ ìë£Œ ê²€ìƒ‰"""
        if self.embedding_model is None or self.index is None:
            logger.error("ë²¡í„° ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        if len(self.documents) == 0:
            logger.error("ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        try:
            # ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = self.embedding_model.encode([query])
            
            # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ ì œí•œ
            search_k = min(n_results * 2, len(self.documents))
            
            # FAISS ê²€ìƒ‰
            distances, indices = self.index.search(query_embedding.astype('float32'), search_k)
            
            # ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if len(distances) == 0 or len(indices) == 0:
                logger.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            # ê²°ê³¼ í•„í„°ë§ ë° í¬ë§·íŒ…
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                # ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ í™•ì¸
                if idx < 0 or idx >= len(self.metadata):
                    continue
                    
                metadata = self.metadata[idx]
                
                # ê³¼ëª© í•„í„°ë§
                if subject and metadata.get("subject") != subject:
                    continue
                
                # í•™ìŠµ ìë£Œ íƒ€ì…ë§Œ í•„í„°ë§
                if metadata.get("type") != "study_material":
                    continue
                
                result = {
                    "id": metadata.get("id"),
                    "content": self.documents[idx],
                    "metadata": metadata,
                    "distance": float(distance),
                    "rank": len(results) + 1
                }
                results.append(result)
                
                if len(results) >= n_results:
                    break
            
            return results
            
        except Exception as e:
            logger.error(f"í•™ìŠµ ìë£Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def get_questions_by_subject(self, subject: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ê³¼ëª©ë³„ ë¬¸ì œ ì¡°íšŒ"""
        results = []
        
        for metadata in self.metadata:
            if (metadata.get("subject") == subject and 
                metadata.get("type") == "exam_question"):
                results.append({
                    "id": metadata.get("id"),
                    "content": self.documents[metadata.get("embedding_id", 0)],
                    "metadata": metadata
                })
                if len(results) >= n_results:
                    break
        
        return results
    
    def get_questions_by_difficulty(self, difficulty: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ë‚œì´ë„ë³„ ë¬¸ì œ ì¡°íšŒ"""
        results = []
        
        for metadata in self.metadata:
            if (metadata.get("difficulty") == difficulty and 
                metadata.get("type") == "exam_question"):
                results.append({
                    "id": metadata.get("id"),
                    "content": self.documents[metadata.get("embedding_id", 0)],
                    "metadata": metadata
                })
                if len(results) >= n_results:
                    break
        
        return results
    
    def get_user_questions(self, user_id: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ìë³„ ì§ˆë¬¸ ì¡°íšŒ"""
        results = []
        
        for metadata in self.metadata:
            if (metadata.get("user_id") == user_id and 
                metadata.get("type") == "user_question"):
                results.append({
                    "id": metadata.get("id"),
                    "content": self.documents[metadata.get("embedding_id", 0)],
                    "metadata": metadata
                })
                if len(results) >= n_results:
                    break
        
        return results
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´"""
        stats = {
            "total_documents": len(self.documents),
            "total_metadata": len(self.metadata),
            "index_size": self.index.ntotal if self.index else 0,
            "exam_questions": len([m for m in self.metadata if m.get("type") == "exam_question"]),
            "study_materials": len([m for m in self.metadata if m.get("type") == "study_material"]),
            "user_questions": len([m for m in self.metadata if m.get("type") == "user_question"]),
            "subjects": list(set([m.get("subject", "") for m in self.metadata if m.get("subject")]))
        }
        return stats
    
    def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ (FAISSì—ì„œëŠ” ë³µì¡í•˜ë¯€ë¡œ ì „ì²´ ì¬êµ¬ì„±)"""
        try:
            # í•´ë‹¹ ë¬¸ì„œ ì°¾ê¸°
            target_idx = None
            for i, metadata in enumerate(self.metadata):
                if metadata.get("id") == doc_id:
                    target_idx = i
                    break
            
            if target_idx is not None:
                # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„° ì œê±°
                del self.documents[target_idx]
                del self.metadata[target_idx]
                
                # FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±
                self._rebuild_index()
                
                logger.info(f"âœ… ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {doc_id}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def delete_exam_data(self, exam_name: str) -> bool:
        """íŠ¹ì • ì‹œí—˜ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        try:
            # í•´ë‹¹ ì‹œí—˜ì˜ ë¬¸ì„œ IDë“¤ ì°¾ê¸°
            doc_ids_to_delete = []
            for meta in self.metadata:
                if meta.get("subject") == exam_name:
                    doc_ids_to_delete.append(meta.get("id"))
            
            if not doc_ids_to_delete:
                logger.info(f"ì‹œí—˜ '{exam_name}'ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return True  # ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ì–´ë„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
            
            # ê° ë¬¸ì„œ ì‚­ì œ
            for doc_id in doc_ids_to_delete:
                self.delete_document(doc_id)
            
            logger.info(f"âœ… ì‹œí—˜ '{exam_name}' ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {len(doc_ids_to_delete)}ê°œ ë¬¸ì„œ")
            return True
            
        except Exception as e:
            logger.error(f"ì‹œí—˜ ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _rebuild_index(self):
        """FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±"""
        if self.embedding_model is None or not self.documents:
            return
        
        try:
            # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ ìƒì„±
            dimension = self.embedding_model.get_sentence_embedding_dimension()
            self.index = faiss.IndexFlatL2(dimension)
            
            # ëª¨ë“  ë¬¸ì„œ ë²¡í„°í™”
            embeddings = self.embedding_model.encode(self.documents)
            self.index.add(embeddings.astype('float32'))
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            for i, metadata in enumerate(self.metadata):
                metadata["embedding_id"] = i
            
            # ì €ì¥
            self._save_data()
            
            logger.info("âœ… FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì¸ë±ìŠ¤ ì¬êµ¬ì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _save_data(self):
        """ë°ì´í„° ì €ì¥"""
        try:
            metadata_file = self.persist_directory / "metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "total_documents": len(self.documents),
                    "metadata": self.metadata,
                    "last_updated": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            if self.index:
                index_file = self.persist_directory / "faiss_index.bin"
                faiss.write_index(self.index, str(index_file))
            
            logger.info("âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def backup_collection(self, backup_path: str):
        """ì»¬ë ‰ì…˜ ë°±ì—…"""
        try:
            backup_data = {
                "total_documents": len(self.documents),
                "documents": self.documents,
                "metadata": self.metadata,
                "backup_timestamp": datetime.now().isoformat()
            }
            
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ë°±ì—… ì™„ë£Œ: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"ë°±ì—… ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def restore_collection(self, backup_path: str):
        """ì»¬ë ‰ì…˜ ë³µì›"""
        try:
            with open(backup_path, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            self.documents = backup_data.get("documents", [])
            self.metadata = backup_data.get("metadata", [])
            
            # FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±
            self._rebuild_index()
            
            logger.info(f"âœ… ë³µì› ì™„ë£Œ: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"ë³µì› ì¤‘ ì˜¤ë¥˜: {e}")
            return False

# ì „ì—­ ë²¡í„° ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
vector_store = VectorStore() 